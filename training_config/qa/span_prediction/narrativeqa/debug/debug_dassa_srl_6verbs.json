// Configuration for the basic QANet model from "QANet: Combining Local
// Convolution with Global Self-Attention for Reading Comprehension"
// (https://arxiv.org/abs/1804.09541).
{
  "dataset_reader": {
    "type": "narrativeqa_summary_with_questions_semantic_optimized",
    "passage_length_limit": 100,
    "question_length_limit": 50,
    "lazy": true,
    "srl_views_extractor": {
      "type": "srl_flat_views",
      "namespace": "srl",
      "max_verbs": 6, //This corresponds to attention heads
    },
    "fit_answer_in_the_passage_limit": true,
    "skip_invalid_examples": true,
    "tokenizer": {
      "type": "word",
      "word_splitter": {
        "type": "read_sentence_parse_typewise",
        "fields": [
          "tokens->text"
        ]
      }
    },
    "token_indexers": {
      "tokens": {
        "type": "single_id",
        "lowercase_tokens": true
      },
      "token_characters": {
        "type": "characters",
        "character_tokenizer": {
          "byte_encoding": "utf-8",
          "start_tokens": [
            259
          ],
          "end_tokens": [
            260
          ]
        },
        "min_padding_length": 5
      }
    }
  },
  "validation_dataset_reader": {
    "type": "narrativeqa_summary_with_questions_semantic_optimized",
    //    "passage_length_limit": 1200,
    //    "question_length_limit": 50,
    "skip_invalid_examples": true,
    "lazy": true,
    "srl_views_extractor": {
      "type": "srl_flat_views",
      "namespace": "srl",
      "max_verbs": 6, //This corresponds to attention heads
    },
    "tokenizer": {
      "type": "word",
      "word_splitter": {
        "type": "read_sentence_parse_typewise",
        "fields": ["tokens->text"]
      }
    },
    "token_indexers": {
      "tokens": {
        "type": "single_id",
        "lowercase_tokens": true
      },
      "token_characters": {
        "type": "characters",
        "character_tokenizer": {
          "byte_encoding": "utf-8",
          "start_tokens": [259],
          "end_tokens": [260]
        },
        "min_padding_length": 5
      }
    }
  },
  "train_data_path": "data/narrativeqa_annotated/summaries_annotated.jsonl.test",
  "validation_data_path": "data/narrativeqa_annotated/summaries_annotated.jsonl.valid",
  "test_data_path": "data/narrativeqa_annotated/summaries_annotated.jsonl.test",
  "evaluate_on_test": true,
  "model": {
    "type": "qanet_semantic_flat",
    "text_field_embedder": {
      "token_embedders": {
        "tokens": {
          "type": "embedding",
          "pretrained_file": "https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz",
          "embedding_dim": 100,
          "trainable": false
        },
        "token_characters": {
          "type": "character_encoding",
          "embedding": {
            "num_embeddings": 262,
            "embedding_dim": 16
          },
          "encoder": {
            "type": "cnn",
            "embedding_dim": 16,
            "num_filters": 100,
            "ngram_filter_sizes": [
              5
            ]
          },
          "dropout": 0.1
        }
      }
    },
    "num_highway_layers": 2,
    "phrase_layer": {
      "type": "qanet_semantic_flat_encoder",
      "input_dim": 200,
      "hidden_dim": 128,
      "attention_projection_dim": 128,
      "feedforward_hidden_dim": 128,
      "num_blocks": 1,
      "num_semantic_labels": 300,
      "replace_zero_semantic_labels_with_per_head_labels": true,
      "num_convs_per_block": 4,
      "conv_kernel_size": 7,
      "num_attention_heads": 8,
      "dropout_prob": 0.1,
      "layer_dropout_undecayed_prob": 0.1,
      "attention_dropout_prob": 0
    },
    "matrix_attention_layer": {
      "type": "linear",
      "tensor_1_dim": 128,
      "tensor_2_dim": 128,
      "combination": "x,y,x*y"
    },
    "modeling_layer": {
      "type": "qanet_encoder",
      "input_dim": 128,
      "hidden_dim": 128,
      "attention_projection_dim": 128,
      "feedforward_hidden_dim": 128,
      "num_blocks": 7,
      "num_convs_per_block": 2,
      "conv_kernel_size": 5,
      "num_attention_heads": 8,
      "dropout_prob": 0.1,
      "layer_dropout_undecayed_prob": 0.1,
      "attention_dropout_prob": 0
    },
    "dropout_prob": 0.1,
    "regularizer": [
      [
        ".*",
        {
          "type": "l2",
          "alpha": 1e-07
        }
      ]
    ]
  },
  "iterator": {
    "type": "bucket",
    "sorting_keys": [["passage", "num_tokens"], ["question", "num_tokens"]],
    "batch_size": std.extVar("BATCH_SIZE")
  },
  "trainer": {
    "num_epochs": 50,
    "grad_norm": 5.0,
    "patience": 10,
    "validation_metric": "+rouge-l",
    "cuda_device": 0,
    "learning_rate_scheduler": {
      "type": "reduce_on_plateau",
      "factor": 0.5,
      "mode": "max",
      "patience": 8
    },
    "optimizer": {
      "type": "adam",
      "lr": 0.001,
      "betas": [
        0.8,
        0.999
      ],
      "eps": 1e-07
    },
//    "moving_average": {
//      "type": "exponential",
//      "decay": 0.9999
//    }
  }
}